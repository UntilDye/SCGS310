import cv2
import torch
import numpy as np
import argparse
import yaml
import json
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings
import tempfile
import os
from PIL import Image
from torchvision import transforms

warnings.filterwarnings('ignore')

from models.yolo_vehicle import VehicleYOLO
from utils.deepsort_tracker import VehicleTracker
from utils.traffic_counter import TrafficCounter

# è½¦ç‰Œç›¸å…³å¯¼å…¥
from models.YOLO2 import YOLO2_MobileNetV2 as YOLO2
from models.Crnn2 import CRNN

class LicensePlateRecognizer:
    """è½¦ç‰Œè¯†åˆ«å™¨ - é›†æˆYOLOæ£€æµ‹å’ŒCRNNè¯†åˆ«"""
    
    def __init__(self, yolo_config_path: str, yolo_model_path: str, crnn_model_path: str, device, debug=False):
        self.device = device
        self.debug = debug
        self.tmp_dir = tempfile.mkdtemp()
        
        # é¦–å…ˆåˆå§‹åŒ–å­—ç¬¦é›†
        self.CHARACTER_SET = self._get_charset()
        self.INDEX_TO_CHAR = {idx + 1: char for idx, char in enumerate(self.CHARACTER_SET)}
        self.INDEX_TO_CHAR[0] = ''  # blank for CTC
        
        if self.debug:
            print(f"[DEBUG] å­—ç¬¦é›†å¤§å°: {len(self.CHARACTER_SET)}")
            print(f"[DEBUG] å­—ç¬¦é›†: {self.CHARACTER_SET}")
        
        # åŠ è½½YOLOè½¦ç‰Œæ£€æµ‹æ¨¡å‹
        self.yolo_model, self.yolo_cfg = self._load_yolo_model(yolo_config_path, yolo_model_path)
        
        # åŠ è½½CRNNè½¦ç‰Œè¯†åˆ«æ¨¡å‹
        self.crnn_model = self._load_crnn_model(crnn_model_path)
        
        print(f"è½¦ç‰Œè¯†åˆ«å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    
    def _get_charset(self):
        """è·å–å­—ç¬¦é›†"""
        provincelist = [
            "çš–", "æ²ª", "æ´¥", "æ¸", "å†€", "æ™‹", "è’™", "è¾½", "å‰", "é»‘", "è‹", "æµ™", "äº¬",
            "é—½", "èµ£", "é²", "è±«", "é„‚", "æ¹˜", "ç²¤", "æ¡‚", "ç¼", "å·", "è´µ", "äº‘", "è¥¿",
            "é™•", "ç”˜", "é’", "å®", "æ–°"
        ]
        wordlist = [
            "A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N",
            "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z",
            "0", "1", "2", "3", "4", "5", "6", "7", "8", "9"
        ]
        merged_string = ''.join(provincelist + wordlist)
        return ''.join(sorted(set(merged_string)))
    
    def _load_yolo_model(self, config_path: str, model_path: str):
        """åŠ è½½YOLOè½¦ç‰Œæ£€æµ‹æ¨¡å‹"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
            if self.debug:
                print(f"[DEBUG] YOLOé…ç½®åŠ è½½æˆåŠŸ: {cfg}")
        except Exception as e:
            print(f"åŠ è½½YOLOé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤é…ç½®
            cfg = {
                'model': {
                    'nc': 1,  # è½¦ç‰Œæ£€æµ‹åªæœ‰ä¸€ä¸ªç±»åˆ«
                    's': 7    # ç½‘æ ¼å¤§å°
                }
            }
            if self.debug:
                print(f"[DEBUG] ä½¿ç”¨é»˜è®¤YOLOé…ç½®: {cfg}")
        
        try:
            model = YOLO2(nc=cfg['model']['nc'], S=cfg['model']['s']).to(self.device)
            model.load_state_dict(torch.load(model_path, map_location=self.device))
            model.eval()
            print(f"YOLOè½¦ç‰Œæ£€æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
            if self.debug:
                print(f"[DEBUG] YOLOæ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
            return model, cfg
        except Exception as e:
            print(f"åŠ è½½YOLOè½¦ç‰Œæ£€æµ‹æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _load_crnn_model(self, model_path: str):
        """åŠ è½½CRNNè½¦ç‰Œè¯†åˆ«æ¨¡å‹"""
        try:
            model = CRNN(num_classes=len(self.CHARACTER_SET)+1, input_channels=1)
            checkpoint = torch.load(model_path, map_location=self.device)
            
            if self.debug:
                print(f"[DEBUG] CRNNæ¨¡å‹ç»“æ„: {model}")
                print(f"[DEBUG] æ£€æŸ¥ç‚¹é”®: {checkpoint.keys() if isinstance(checkpoint, dict) else 'éå­—å…¸æ ¼å¼'}")
            
            # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                elif 'state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['state_dict'])
                else:
                    model.load_state_dict(checkpoint)
            else:
                model.load_state_dict(checkpoint)
                
            model = model.to(self.device)
            model.eval()
            print(f"CRNNè½¦ç‰Œè¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸ")
            if self.debug:
                print(f"[DEBUG] CRNNæ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
            return model
        except Exception as e:
            print(f"åŠ è½½CRNNè½¦ç‰Œè¯†åˆ«æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _preprocess_yolo_image(self, img_array: np.ndarray, img_size: int):
        """é¢„å¤„ç†å›¾åƒç”¨äºYOLOæ£€æµ‹"""
        try:
            if self.debug:
                print(f"[DEBUG] YOLOé¢„å¤„ç†è¾“å…¥: shape={img_array.shape}, size={img_size}")
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            if len(img_array.shape) == 3:
                img_pil = Image.fromarray(cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB))
            else:
                img_pil = Image.fromarray(img_array)
            
            # é¢„å¤„ç†
            transform = transforms.Compose([
                transforms.Resize((img_size, img_size)),
                transforms.ToTensor()
            ])
            img_tensor = transform(img_pil).unsqueeze(0)
            
            if self.debug:
                print(f"[DEBUG] YOLOé¢„å¤„ç†è¾“å‡º: PIL size={img_pil.size}, tensor shape={img_tensor.shape}")
            
            return img_pil, img_tensor
        except Exception as e:
            print(f"YOLOå›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _yolo_box_to_img_coords(self, cell_row, cell_col, x, y, w, h, S, net_w, net_h, img_w, img_h):
        """YOLOåæ ‡è½¬æ¢"""
        try:
            x_center = ((cell_col + x) / S) * net_w
            y_center = ((cell_row + y) / S) * net_h
            bw = w * net_w
            bh = h * net_h
            
            scale_x = img_w / net_w
            scale_y = img_h / net_h
            
            x_center *= scale_x
            y_center *= scale_y
            bw *= scale_x
            bh *= scale_y
            
            x1 = x_center - bw / 2
            y1 = y_center - bh / 2
            x2 = x_center + bw / 2
            y2 = y_center + bh / 2
            
            if self.debug:
                print(f"[DEBUG] YOLOåæ ‡è½¬æ¢: ({cell_row},{cell_col}) -> ({x1:.1f},{y1:.1f},{x2:.1f},{y2:.1f})")
            
            return [x1, y1, x2, y2]
        except Exception as e:
            print(f"YOLOåæ ‡è½¬æ¢å¤±è´¥: {e}")
            return [0, 0, 100, 50]  # è¿”å›é»˜è®¤å€¼
    
    def _detect_license_plate(self, img_array: np.ndarray, threshold: float = 0.2):
        """ä½¿ç”¨YOLOæ£€æµ‹è½¦ç‰Œ"""
        try:
            if self.debug:
                print(f"[DEBUG] å¼€å§‹è½¦ç‰Œæ£€æµ‹ï¼Œè¾“å…¥shape: {img_array.shape}, é˜ˆå€¼: {threshold}")
            
            img_size = self.yolo_cfg.get('training', {}).get('img_size', 416)
            if isinstance(img_size, list):
                img_size = img_size[0]
            
            img_pil, img_tensor = self._preprocess_yolo_image(img_array, img_size)
            img_tensor = img_tensor.to(self.device)
            
            with torch.no_grad():
                pred = self.yolo_model(img_tensor)  # [1, S, S, 5]
                pred = pred[0].cpu()      # [S, S, 5]
                S = pred.shape[0]
                
                conf = torch.sigmoid(pred[..., 0])
                best = torch.argmax(conf)
                i, j = np.unravel_index(best, (S, S))
                
                max_confidence = conf[i, j].item()
                
                if self.debug:
                    print(f"[DEBUG] YOLOé¢„æµ‹: S={S}, æœ€é«˜ç½®ä¿¡åº¦={max_confidence:.3f} at ({i},{j})")
                
                if max_confidence < threshold:
                    if self.debug:
                        print(f"[DEBUG] ç½®ä¿¡åº¦{max_confidence:.3f}ä½äºé˜ˆå€¼{threshold}ï¼Œæœªæ£€æµ‹åˆ°è½¦ç‰Œ")
                    return None
                
                x, y, w, h = pred[i, j, 1:5].numpy()
                net_w, net_h = img_tensor.shape[-1], img_tensor.shape[-2]
                img_w, img_h = img_pil.size
                
                x1, y1, x2, y2 = self._yolo_box_to_img_coords(
                    i, j, x, y, w, h, S, net_w, net_h, img_w, img_h
                )
                
                # åæ ‡æ ¡æ­£
                x1, y1 = max(0, int(x1)), max(0, int(y1))
                x2, y2 = min(img_w, int(x2)), min(img_h, int(y2))
                
                # æ£€æŸ¥è¾¹ç•Œæ¡†æœ‰æ•ˆæ€§
                if x2 <= x1 or y2 <= y1:
                    if self.debug:
                        print(f"[DEBUG] æ— æ•ˆè¾¹ç•Œæ¡†: ({x1},{y1},{x2},{y2})")
                    return None
                
                if self.debug:
                    print(f"[DEBUG] æ£€æµ‹åˆ°è½¦ç‰Œ: bbox=({x1},{y1},{x2},{y2}), ç½®ä¿¡åº¦={max_confidence:.3f}")
                
                return (x1, y1, x2, y2)
                
        except Exception as e:
            print(f"è½¦ç‰Œæ£€æµ‹å¤±è´¥: {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return None
    
    def _crop_license_plate(self, img_array: np.ndarray, bbox: Tuple[int, int, int, int]):
        """è£å‰ªè½¦ç‰ŒåŒºåŸŸ"""
        try:
            x1, y1, x2, y2 = bbox
            h, w = img_array.shape[:2]
            
            if self.debug:
                print(f"[DEBUG] è£å‰ªè½¦ç‰Œ: åŸå›¾{w}x{h}, bbox=({x1},{y1},{x2},{y2})")
            
            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
            x1 = max(0, min(x1, w-1))
            y1 = max(0, min(y1, h-1))
            x2 = max(x1+1, min(x2, w))
            y2 = max(y1+1, min(y2, h))
            
            cropped = img_array[y1:y2, x1:x2]
            
            # æ£€æŸ¥è£å‰ªç»“æœ
            if cropped.size == 0:
                if self.debug:
                    print(f"[DEBUG] è£å‰ªç»“æœä¸ºç©º")
                return None
            
            if self.debug:
                print(f"[DEBUG] è£å‰ªæˆåŠŸ: {cropped.shape}")
                
            return cropped
        except Exception as e:
            print(f"è½¦ç‰Œè£å‰ªå¤±è´¥: {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return None
    
    def _get_infer_transform(self, img_size=(32, 320)):
        """è·å–CRNNæ¨ç†é¢„å¤„ç†å˜æ¢"""
        return transforms.Compose([
            transforms.Resize(img_size),
            transforms.Grayscale(num_output_channels=1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5], std=[0.5]),
        ])
    
    def _decode_crnn_output(self, preds):
        """è§£ç CRNNè¾“å‡º"""
        try:
            if self.debug:
                print(f"[DEBUG] CRNNè¾“å‡ºshape: {preds.shape}")
            
            pred_indices = preds.argmax(dim=2)
            pred_indices = pred_indices.permute(1, 0)
            results = []
            
            if self.debug:
                print(f"[DEBUG] é¢„æµ‹ç´¢å¼•shape: {pred_indices.shape}")
            
            for batch_idx, indices in enumerate(pred_indices):
                prev_idx = -1
                text = ''
                char_details = []
                
                for pos, idx in enumerate(indices):
                    idx = idx.item()
                    char = self.INDEX_TO_CHAR.get(idx, '')
                    
                    if self.debug:
                        char_details.append(f"pos{pos}:idx{idx}->'{char}'")
                    
                    if idx != 0 and idx != prev_idx:  # ä¸æ˜¯ç©ºç™½ä¸”ä¸é‡å¤
                        if char:  # åªæ·»åŠ æœ‰æ•ˆå­—ç¬¦
                            text += char
                    prev_idx = idx
                
                if self.debug:
                    print(f"[DEBUG] æ‰¹æ¬¡{batch_idx} å­—ç¬¦è¯¦æƒ…: {char_details[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
                    print(f"[DEBUG] æ‰¹æ¬¡{batch_idx} æœ€ç»ˆæ–‡æœ¬: '{text}'")
                
                results.append(text)
            
            return results
        except Exception as e:
            print(f"CRNNè¾“å‡ºè§£ç å¤±è´¥: {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return [""]
    
    def _recognize_license_plate(self, cropped_img: np.ndarray):
        """ä½¿ç”¨CRNNè¯†åˆ«è½¦ç‰Œæ–‡å­—"""
        try:
            if cropped_img is None or cropped_img.size == 0:
                if self.debug:
                    print(f"[DEBUG] è£å‰ªå›¾åƒæ— æ•ˆ")
                return ""
            
            if self.debug:
                print(f"[DEBUG] å¼€å§‹CRNNè¯†åˆ«ï¼Œè¾“å…¥shape: {cropped_img.shape}")
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            if len(cropped_img.shape) == 3:
                img_pil = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))
            else:
                img_pil = Image.fromarray(cropped_img)
            
            if self.debug:
                print(f"[DEBUG] PILå›¾åƒè½¬æ¢å®Œæˆ: {img_pil.size}")
            
            # é¢„å¤„ç†
            transform = self._get_infer_transform()
            image = transform(img_pil).unsqueeze(0).to(self.device)
            
            if self.debug:
                print(f"[DEBUG] CRNNè¾“å…¥tensor shape: {image.shape}")
            
            # æ¨ç†
            with torch.no_grad():
                output = self.crnn_model(image)
                output = output.permute(1, 0, 2)
                
                if self.debug:
                    print(f"[DEBUG] CRNNåŸå§‹è¾“å‡ºshape: {output.shape}")
            
            # è§£ç 
            texts = self._decode_crnn_output(output)
            result_text = texts[0] if texts else ""
            
            if self.debug:
                print(f"[DEBUG] CRNNæœ€ç»ˆè¯†åˆ«ç»“æœ: '{result_text}'")
            
            return result_text
            
        except Exception as e:
            print(f"è½¦ç‰Œè¯†åˆ«å‡ºé”™: {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return ""
    
    def recognize_from_vehicle_crop(self, vehicle_img: np.ndarray):
        """ä»è½¦è¾†è£å‰ªå›¾åƒä¸­è¯†åˆ«è½¦ç‰Œ"""
        try:
            if vehicle_img is None or vehicle_img.size == 0:
                if self.debug:
                    print(f"[DEBUG] è½¦è¾†å›¾åƒæ— æ•ˆ")
                return None
            
            if self.debug:
                print(f"[DEBUG] å¼€å§‹ä»è½¦è¾†å›¾åƒè¯†åˆ«è½¦ç‰Œï¼Œè¾“å…¥shape: {vehicle_img.shape}")
            
            # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹è½¦ç‰Œä½ç½®
            bbox = self._detect_license_plate(vehicle_img, threshold=0.2)
            if bbox is None:
                if self.debug:
                    print(f"[DEBUG] æœªæ£€æµ‹åˆ°è½¦ç‰Œä½ç½®")
                return None
            
            # ç¬¬äºŒæ­¥ï¼šè£å‰ªè½¦ç‰ŒåŒºåŸŸ
            license_plate_crop = self._crop_license_plate(vehicle_img, bbox)
            if license_plate_crop is None:
                if self.debug:
                    print(f"[DEBUG] è½¦ç‰ŒåŒºåŸŸè£å‰ªå¤±è´¥")
                return None
            
            # ç¬¬ä¸‰æ­¥ï¼šè¯†åˆ«è½¦ç‰Œæ–‡å­—
            license_text = self._recognize_license_plate(license_plate_crop)
            
            if self.debug:
                print(f"[DEBUG] è½¦ç‰Œè¯†åˆ«å®Œæˆ: '{license_text}', é•¿åº¦: {len(license_text.strip())}")
            
            # è¿‡æ»¤æ— æ•ˆç»“æœ
            if license_text and len(license_text.strip()) >= 6:
                result = {
                    'bbox': bbox,
                    'text': license_text.strip(),
                    'confidence': 0.8  # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                }
                if self.debug:
                    print(f"[DEBUG] è½¦ç‰Œè¯†åˆ«æˆåŠŸ: {result}")
                return result
            else:
                if self.debug:
                    print(f"[DEBUG] è½¦ç‰Œæ–‡æœ¬é•¿åº¦ä¸è¶³æˆ–ä¸ºç©ºï¼Œè¢«è¿‡æ»¤")
            
            return None
            
        except Exception as e:
            print(f"è½¦ç‰Œè¯†åˆ«è¿‡ç¨‹å‡ºé”™: {e}")
            if self.debug:
                import traceback
                traceback.print_exc()
            return None
    
    def __del__(self):
        """æ¸…ç†ä¸´æ—¶ç›®å½•"""
        try:
            import shutil
            if hasattr(self, 'tmp_dir') and os.path.exists(self.tmp_dir):
                shutil.rmtree(self.tmp_dir)
        except:
            pass

class VideoVehicleAnalyzerWithLicensePlate:
    """å¸¦è½¦ç‰Œè¯†åˆ«çš„è§†é¢‘è½¦è¾†åˆ†æå™¨"""
    
    def __init__(self, config_path: str, vehicle_model_path: str, 
             license_yolo_config: str, license_yolo_model: str, 
             license_crnn_model: str, debug: bool = False):
    
        self.debug = debug
        
        # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡æ—¶ç¡®ä¿ç±»å‹æ­£ç¡®
        self.total_license_attempts = 0
        self.successful_license_recognitions = 0
        self.recognized_licenses = set()
        self.license_output_log = []
        self.license_cache = {}
        self.license_cache_ttl = 30
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        self.class_names = self.config['model']['class_names']
        
        # åŠ è½½è½¦è¾†æ£€æµ‹æ¨¡å‹
        self.vehicle_model = self._load_vehicle_model(vehicle_model_path)
        
        # åˆå§‹åŒ–è½¦ç‰Œè¯†åˆ«å™¨
        try:
            self.license_recognizer = LicensePlateRecognizer(
                license_yolo_config, license_yolo_model, license_crnn_model, 
                self.device, debug=self.debug
            )
            self.license_enabled = True
            print(f"è½¦ç‰Œè¯†åˆ«åŠŸèƒ½å·²å¯ç”¨")
        except Exception as e:
            print(f"è½¦ç‰Œè¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print("å°†ç»§ç»­ä½¿ç”¨è½¦è¾†æ£€æµ‹åŠŸèƒ½ï¼Œä½†è½¦ç‰Œè¯†åˆ«åŠŸèƒ½å°†è¢«ç¦ç”¨")
            if self.debug:
                import traceback
                traceback.print_exc()
            self.license_recognizer = None
            self.license_enabled = False
        
        # è½¦è¾†æ£€æµ‹å‚æ•°
        detection_cfg = self.config.get('detection', {})
        self.conf_threshold = float(detection_cfg.get('conf_threshold', 0.15))
        self.iou_threshold = float(detection_cfg.get('iou_threshold', 0.45))
        self.nms_iou_threshold = float(detection_cfg.get('nms_iou_threshold', 0.5))
        
        # è¿‡æ»¤å‚æ•°
        filtering_cfg = self.config.get('filtering', {})
        self.min_box_area = filtering_cfg.get('min_box_area', 100)
        self.max_box_area = filtering_cfg.get('max_box_area', 500000)
        self.min_aspect_ratio = filtering_cfg.get('min_aspect_ratio', 0.05)
        self.max_aspect_ratio = filtering_cfg.get('max_aspect_ratio', 20.0)
        
        # è¿½è¸ªå™¨é…ç½®
        tracker_config = self.config.get('tracking', {})
        self.tracker = VehicleTracker(
            class_names=self.class_names,
            max_age=30,
            n_init=1,
            max_iou_distance=0.7,
            debug=debug
        )
        
        # æµé‡è®¡æ•°å™¨
        counting_config = self.config.get('counting', {})
        counting_lines_config = counting_config.get('counting_line', [[[300, 400], [900, 400]]])
        count_direction = counting_config.get('count_direction', 'both')
        
        formatted_lines = self._parse_counting_lines(counting_lines_config)
        self.traffic_counter = TrafficCounter(formatted_lines, count_direction)
        
        # å¯è§†åŒ–é¢œè‰²
        self.colors = [
            (255, 56, 56),   # çº¢è‰² (æ‘©æ‰˜è½¦)
            (50, 205, 50),   # ç»¿è‰² (æ±½è½¦)
            (70, 130, 180),  # è“è‰² (å·´å£«)
            (255, 165, 0),   # æ©™è‰² (å¡è½¦)
        ]
        
        # è½¦ç‰Œè¯†åˆ«ç¼“å­˜ - é¿å…é‡å¤è¯†åˆ«
        self.license_cache = {}
        self.license_cache_ttl = 30  # ç¼“å­˜30å¸§
        
        # æ·»åŠ è½¦ç‰Œè¾“å‡ºç›¸å…³å˜é‡
        self.recognized_licenses = set()  # å­˜å‚¨å·²è¯†åˆ«çš„è½¦ç‰Œï¼Œé¿å…é‡å¤è¾“å‡º
        self.license_output_log = []  # å­˜å‚¨è½¦ç‰Œè¾“å‡ºæ—¥å¿—
        
        # ç»Ÿè®¡å˜é‡
        self.total_license_attempts = 0  # å°è¯•è¯†åˆ«è½¦ç‰Œçš„æ€»æ¬¡æ•°
        self.successful_license_recognitions = 0  # æˆåŠŸè¯†åˆ«çš„æ¬¡æ•°
    
    def _load_vehicle_model(self, model_path: str):
        """åŠ è½½è½¦è¾†æ£€æµ‹æ¨¡å‹"""
        print(f"åŠ è½½è½¦è¾†æ£€æµ‹æ¨¡å‹: {model_path}")
        
        input_size_cfg = self.config['model']['input_size']
        try:
            input_shape = (int(input_size_cfg[0]), int(input_size_cfg[1]))
        except (TypeError, IndexError, ValueError) as e:
            print(f"ä»é…ç½®è§£æ input_size æ—¶å‡ºé”™: {input_size_cfg}. ä½¿ç”¨é»˜è®¤ (640, 640)")
            input_shape = (640, 640)
        
        model = VehicleYOLO(
            num_classes=int(self.config['model']['num_classes']),
            input_shape=input_shape
        )
        
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            elif 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'])
            else:
                model.load_state_dict(checkpoint)
        except Exception as e:
            print(f"åŠ è½½è½¦è¾†æ£€æµ‹æ¨¡å‹æƒé‡æ—¶å‡ºé”™: {e}")
            raise
        
        model.to(self.device)
        model.eval()
        print("è½¦è¾†æ£€æµ‹æ¨¡å‹åŠ è½½å®Œæˆ")
        return model
    
    def _parse_counting_lines(self, counting_lines_config):
        """è§£æè®¡æ•°çº¿é…ç½®"""
        formatted_lines = []
        if isinstance(counting_lines_config, list) and len(counting_lines_config) > 0:
            for line_coords in counting_lines_config:
                try:
                    if (isinstance(line_coords, list) and len(line_coords) == 2 and
                        isinstance(line_coords[0], list) and len(line_coords[0]) == 2 and
                        isinstance(line_coords[1], list) and len(line_coords[1]) == 2):
                        
                        p1 = (int(line_coords[0][0]), int(line_coords[0][1]))
                        p2 = (int(line_coords[1][0]), int(line_coords[1][1]))
                        formatted_lines.append([p1, p2])
                    else:
                        print(f"è­¦å‘Š: è®¡æ•°çº¿æ ¼å¼ä¸æ­£ç¡®: {line_coords}. ä½¿ç”¨é»˜è®¤è®¡æ•°çº¿.")
                        formatted_lines = [[(300, 400), (900, 400)]]
                        break
                except (IndexError, TypeError, ValueError) as e:
                    print(f"è§£æè®¡æ•°çº¿é…ç½®æ—¶å‡ºé”™: {e}. ä½¿ç”¨é»˜è®¤è®¡æ•°çº¿.")
                    formatted_lines = [[(300, 400), (900, 400)]]
                    break
        else:
            formatted_lines = [[(300, 400), (900, 400)]]
        
        return formatted_lines
    
    def detect_vehicles(self, frame: np.ndarray) -> List[List]:
        """è½¦è¾†æ£€æµ‹æ–¹æ³•"""
        img_orig_h, img_orig_w = frame.shape[:2]
        
        # é¢„å¤„ç†
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        input_size_cfg = self.config['model']['input_size']
        input_w, input_h = int(input_size_cfg[0]), int(input_size_cfg[1])
        img_resized = cv2.resize(img_rgb, (input_w, input_h), interpolation=cv2.INTER_LINEAR)
        
        # å½’ä¸€åŒ–
        img_tensor = torch.from_numpy(img_resized).float().permute(2, 0, 1) / 255.0
        mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(3, 1, 1)
        img_tensor = (img_tensor.to(self.device) - mean) / std
        img_tensor = img_tensor.unsqueeze(0)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            try:
                if hasattr(self.vehicle_model, 'predict') and callable(self.vehicle_model.predict):
                    raw_preds = self.vehicle_model.predict(
                        img_tensor,
                        conf_threshold=self.conf_threshold,
                        iou_threshold=self.iou_threshold,
                        device=self.device
                    )
                else:
                    raw_preds = self.vehicle_model(img_tensor)
            except Exception as e:
                print(f"è½¦è¾†æ£€æµ‹æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
                return []
        
        # åå¤„ç†å’Œè¿‡æ»¤
        results = self._post_process_and_filter(
            raw_preds,
            (img_orig_w, img_orig_h),
            (input_w, input_h)
        )
        
        return results
    
    def _post_process_and_filter(self, detections, original_size, input_size):
        """åå¤„ç†å’Œè¿‡æ»¤æ–¹æ³•ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
        if not detections:
            return []

        original_width, original_height = original_size
        input_width, input_height = input_size
        scale_x = original_width / input_width
        scale_y = original_height / input_height

        detection = detections[0] if isinstance(detections, list) else detections
        if not isinstance(detection, dict):
            return []

        boxes = detection.get('boxes', [])
        scores = detection.get('scores', [])
        class_ids = detection.get('class_ids', [])

        if not (len(boxes) == len(scores) == len(class_ids)):
            return []

        filtered_detections = []
        
        for i, (box, score, class_id) in enumerate(zip(boxes, scores, class_ids)):
            # è½¬æ¢tensor
            if isinstance(box, torch.Tensor):
                box = box.cpu().numpy()
            if isinstance(score, torch.Tensor):
                score = score.cpu().item()
            if isinstance(class_id, torch.Tensor):
                class_id = class_id.cpu().item()

            # ç½®ä¿¡åº¦è¿‡æ»¤
            if score < self.conf_threshold:
                continue

            try:
                if len(box) != 4:
                    continue
                
                # åæ ‡è½¬æ¢é€»è¾‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                max_coord = float(np.max(box))
                if 0.0 < max_coord <= 1.0:
                    # å½’ä¸€åŒ–åæ ‡
                    cx_norm, cy_norm, w_norm, h_norm = box
                    
                    center_x_orig = cx_norm * original_width
                    center_y_orig = cy_norm * original_height
                    w_orig = w_norm * original_width
                    h_orig = h_norm * original_height
                    
                    x1 = center_x_orig - w_orig / 2.0
                    y1 = center_y_orig - h_orig / 2.0
                    x2 = center_x_orig + w_orig / 2.0
                    y2 = center_y_orig + h_orig / 2.0
                else:
                    # è¾“å…¥å°ºå¯¸åæ ‡
                    if max_coord > input_width:
                        x1, y1, x2, y2 = box
                        x1 *= scale_x
                        y1 *= scale_y
                        x2 *= scale_x
                        y2 *= scale_y
                    else:
                        cx_input, cy_input, w_input, h_input = box
                        center_x_orig = cx_input * scale_x
                        center_y_orig = cy_input * scale_y
                        w_orig = w_input * scale_x
                        h_orig = h_input * scale_y
                        
                        x1 = center_x_orig - w_orig / 2.0
                        y1 = center_y_orig - h_orig / 2.0
                        x2 = center_x_orig + w_orig / 2.0
                        y2 = center_y_orig + h_orig / 2.0

                # é™åˆ¶åœ¨å›¾åƒèŒƒå›´å†…
                x1 = max(0.0, min(x1, original_width - 1))
                y1 = max(0.0, min(y1, original_height - 1))
                x2 = max(x1 + 10, min(x2, original_width))
                y2 = max(y1 + 10, min(y2, original_height))

                width = x2 - x1
                height = y2 - y1
                
                # å‡ ä½•è¿‡æ»¤
                area = width * height
                aspect_ratio = width / height if height > 0 else 0
                
                if (area >= self.min_box_area and area <= self.max_box_area and
                    aspect_ratio >= self.min_aspect_ratio and aspect_ratio <= self.max_aspect_ratio and
                    width >= 10 and height >= 10):
                    
                    # ç±»åˆ«è¿‡æ»¤
                    if 0 <= class_id < len(self.class_names):
                        filtered_detections.append({
                            'bbox': [float(x1), float(y1), float(x2), float(y2)],
                            'score': float(score),
                            'class_id': int(class_id),
                            'area': area
                        })

            except Exception as e:
                if self.debug:
                    print(f"å¤„ç†æ£€æµ‹æ¡† {i} æ—¶å‡ºé”™: {e}")
                continue

        # NMSå¤„ç†
        if not filtered_detections:
            return []
        
        final_detections = []
        for class_id in set(det['class_id'] for det in filtered_detections):
            class_detections = [det for det in filtered_detections if det['class_id'] == class_id]
            
            if class_detections:
                class_detections.sort(key=lambda x: x['score'], reverse=True)
                
                boxes_np = np.array([det['bbox'] for det in class_detections])
                scores_np = np.array([det['score'] for det in class_detections])
                
                indices = cv2.dnn.NMSBoxes(
                    boxes_np.tolist(),
                    scores_np.tolist(),
                    self.conf_threshold * 0.8,
                    self.nms_iou_threshold
                )
                
                if len(indices) > 0:
                    indices = indices.flatten()
                    for idx in indices:
                        det = class_detections[idx]
                        final_detections.append([
                            det['bbox'][0], det['bbox'][1], det['bbox'][2], det['bbox'][3],
                            det['score'], det['class_id']
                        ])

        return final_detections
    
    def _recognize_license_plates(self, frame: np.ndarray, tracked_objects: List, frame_count: int):
        """è¯†åˆ«è½¦ç‰Œå¹¶æ·»åŠ åˆ°è¿½è¸ªå¯¹è±¡ä¸­"""
        if not self.license_enabled:
            return
            
        for obj in tracked_objects:
            # ç¡®ä¿ track_id æ˜¯æ•´æ•°ç±»å‹
            track_id = obj.get('track_id', 0)
            try:
                track_id = int(track_id) if not isinstance(track_id, int) else track_id
            except (ValueError, TypeError):
                if self.debug:
                    print(f"[DEBUG] æ— æ•ˆçš„ track_id: {obj.get('track_id')}, è·³è¿‡è½¦ç‰Œè¯†åˆ«")
                continue
            
            # æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é¢‘ç¹è¯†åˆ«
            cache_key = f"{track_id}_{frame_count // self.license_cache_ttl}"
            if cache_key in self.license_cache:
                obj['license_plate'] = self.license_cache[cache_key]
                if self.debug:
                    print(f"[DEBUG] è½¦è¾†{track_id}ä½¿ç”¨ç¼“å­˜çš„è½¦ç‰Œ: {self.license_cache[cache_key]}")
                continue
            
            # è½¦è¾†æ£€æµ‹ç½®ä¿¡åº¦è¿‡æ»¤ - åªå¯¹é«˜ç½®ä¿¡åº¦çš„è½¦è¾†è¿›è¡Œè½¦ç‰Œè¯†åˆ«
            confidence = obj.get('confidence', 0)
            try:
                confidence = float(confidence) if confidence is not None else 0.0
            except (ValueError, TypeError):
                confidence = 0.0
                
            if confidence < 0.5:
                if self.debug:
                    print(f"[DEBUG] è½¦è¾†{track_id}ç½®ä¿¡åº¦{confidence:.2f}è¿‡ä½ï¼Œè·³è¿‡è½¦ç‰Œè¯†åˆ«")
                continue
            
            self.total_license_attempts += 1
            
            try:
                # è£å‰ªè½¦è¾†åŒºåŸŸ
                bbox = obj.get('bbox', [0, 0, 100, 100])
                if not isinstance(bbox, (list, tuple)) or len(bbox) != 4:
                    if self.debug:
                        print(f"[DEBUG] è½¦è¾†{track_id}è¾¹ç•Œæ¡†æ— æ•ˆ: {bbox}")
                    continue
                    
                x1, y1, x2, y2 = [int(float(c)) for c in bbox]  # ç¡®ä¿åæ ‡æ˜¯æ•´æ•°
                
                if self.debug:
                    print(f"[DEBUG] è½¦è¾†{track_id}è¾¹ç•Œæ¡†: ({x1},{y1},{x2},{y2})")
                
                # æ‰©å±•è¾¹ç•Œæ¡†ä»¥åŒ…å«æ›´å¤šè½¦ç‰ŒåŒºåŸŸ
                margin_x = max(1, int((x2 - x1) * 0.1))
                margin_y = max(1, int((y2 - y1) * 0.1))
                
                x1_expanded = max(0, x1 - margin_x)
                y1_expanded = max(0, y1 - margin_y)
                x2_expanded = min(frame.shape[1], x2 + margin_x)
                y2_expanded = min(frame.shape[0], y2 + margin_y)
                
                vehicle_crop = frame[y1_expanded:y2_expanded, x1_expanded:x2_expanded]
                
                if self.debug:
                    print(f"[DEBUG] è½¦è¾†{track_id}æ‰©å±•ååŒºåŸŸ: ({x1_expanded},{y1_expanded},{x2_expanded},{y2_expanded})")
                    print(f"[DEBUG] è½¦è¾†{track_id}è£å‰ªåŒºåŸŸshape: {vehicle_crop.shape}")
                
                # æ£€æŸ¥è£å‰ªåŒºåŸŸæ˜¯å¦æœ‰æ•ˆ
                if vehicle_crop.size == 0 or vehicle_crop.shape[0] < 30 or vehicle_crop.shape[1] < 30:
                    if self.debug:
                        print(f"[DEBUG] è½¦è¾†{track_id}è£å‰ªåŒºåŸŸå¤ªå°ï¼Œè·³è¿‡è½¦ç‰Œè¯†åˆ«")
                    continue
                
                # è½¦ç‰Œè¯†åˆ«
                license_result = self.license_recognizer.recognize_from_vehicle_crop(vehicle_crop)
                
                if self.debug:
                    print(f"[DEBUG] è½¦è¾†{track_id}è½¦ç‰Œè¯†åˆ«åŸå§‹ç»“æœ: {license_result}")
                
                if license_result and license_result.get('text'):
                    # è¿‡æ»¤æ˜æ˜¾é”™è¯¯çš„è½¦ç‰Œç»“æœ
                    license_text = str(license_result['text']).strip()  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                    
                    if self.debug:
                        print(f"[DEBUG] è½¦è¾†{track_id}è¯†åˆ«åˆ°è½¦ç‰Œæ–‡æœ¬: '{license_text}', é•¿åº¦: {len(license_text)}")
                    
                    if len(license_text) >= 6:  # è‡³å°‘6ä¸ªå­—ç¬¦
                        obj['license_plate'] = license_result
                        self.license_cache[cache_key] = license_result
                        self.successful_license_recognitions += 1
                        
                        # è¾“å‡ºè½¦ç‰Œåˆ°æ§åˆ¶å°ï¼ˆé¿å…é‡å¤è¾“å‡ºï¼‰
                        self._output_license_to_console(track_id, license_text, frame_count, obj)
                    else:
                        if self.debug:
                            print(f"[DEBUG] è½¦è¾†{track_id}è½¦ç‰Œæ–‡æœ¬å¤ªçŸ­ï¼Œè¢«è¿‡æ»¤: '{license_text}'")
                else:
                    if self.debug:
                        print(f"[DEBUG] è½¦è¾†{track_id}æœªè¯†åˆ«åˆ°æœ‰æ•ˆè½¦ç‰Œ")
                    
            except Exception as e:
                print(f"è½¦è¾†{track_id}è½¦ç‰Œè¯†åˆ«å‡ºé”™: {e}")
                if self.debug:
                    import traceback
                    traceback.print_exc()
                continue
        
    def _output_license_to_console(self, track_id: int, license_text: str, frame_count: int, vehicle_obj: dict):
        """è¾“å‡ºè½¦ç‰Œä¿¡æ¯åˆ°æ§åˆ¶å°"""
        # æ£€æŸ¥è½¦ç‰Œæ–‡æœ¬æ˜¯å¦æœ‰æ•ˆ
        if not license_text or not license_text.strip():
            if self.debug:
                print(f"[DEBUG] è­¦å‘Š: è½¦è¾† {track_id} çš„è½¦ç‰Œæ–‡æœ¬ä¸ºç©º")
            return
        
        license_text = license_text.strip()
        
        # ç¡®ä¿ track_id æ˜¯æ•´æ•°ç±»å‹
        try:
            track_id = int(track_id) if not isinstance(track_id, int) else track_id
        except (ValueError, TypeError):
            track_id = 0
            if self.debug:
                print(f"[DEBUG] è­¦å‘Š: track_id è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0")
        
        # åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦ï¼Œé¿å…åŒä¸€è½¦è¾†çš„åŒä¸€è½¦ç‰Œé‡å¤è¾“å‡º
        license_key = f"{track_id}_{license_text}"
        
        if license_key not in self.recognized_licenses:
            self.recognized_licenses.add(license_key)
            
            # è·å–å½“å‰æ—¶é—´æˆ³
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            
            # è·å–è½¦è¾†ä¿¡æ¯ - æ·»åŠ ç±»å‹æ£€æŸ¥å’Œé»˜è®¤å€¼
            vehicle_class = vehicle_obj.get('class_id', -1)
            try:
                vehicle_class = int(vehicle_class) if vehicle_class is not None else -1
            except (ValueError, TypeError):
                vehicle_class = -1
                
            vehicle_class_name = self.class_names[vehicle_class] if 0 <= vehicle_class < len(self.class_names) else "æœªçŸ¥"
            
            vehicle_confidence = vehicle_obj.get('confidence', 0.0)
            try:
                vehicle_confidence = float(vehicle_confidence) if vehicle_confidence is not None else 0.0
            except (ValueError, TypeError):
                vehicle_confidence = 0.0
                
            vehicle_bbox = vehicle_obj.get('bbox', [0, 0, 0, 0])
            if not isinstance(vehicle_bbox, (list, tuple)) or len(vehicle_bbox) != 4:
                vehicle_bbox = [0, 0, 0, 0]
            
            # ç¡®ä¿ frame_count æ˜¯æ•´æ•°
            try:
                frame_count = int(frame_count) if not isinstance(frame_count, int) else frame_count
            except (ValueError, TypeError):
                frame_count = 0
            
            # çªå‡ºæ˜¾ç¤ºçš„æ§åˆ¶å°è¾“å‡º
            print("\n" + "="*70)
            print("ğŸš— è½¦ç‰Œè¯†åˆ«æˆåŠŸ!")
            print("="*70)
            print(f"   æ—¶é—´: {timestamp}")
            print(f"   è½¦è¾†ID: {track_id:03d}")  # ç¡®ä¿ track_id æ˜¯æ•´æ•°
            print(f"   è½¦è¾†ç±»å‹: {vehicle_class_name}")
            print(f"   è½¦è¾†ç½®ä¿¡åº¦: {vehicle_confidence:.3f}")
            print(f"   è½¦è¾†ä½ç½®: [{int(vehicle_bbox[0])}, {int(vehicle_bbox[1])}, {int(vehicle_bbox[2])}, {int(vehicle_bbox[3])}]")
            print(f"   ğŸ·ï¸  è½¦ç‰Œå·: ã€{license_text}ã€‘")  # ç”¨ã€ã€‘åŒ…å›´è½¦ç‰Œå·å¢åŠ å¯è§æ€§
            print(f"   æ£€æµ‹å¸§: {frame_count}")
            print(f"   è½¦ç‰Œé•¿åº¦: {len(license_text)} å­—ç¬¦")
            
            # è½¦ç‰Œè´¨é‡è¯„ä¼°
            quality_score = self._assess_license_quality(license_text)
            print(f"   è½¦ç‰Œè´¨é‡: {quality_score}")
            
            print("="*70)
            
            # ä¿å­˜åˆ°æ—¥å¿—
            log_entry = {
                'timestamp': timestamp,
                'track_id': track_id,
                'vehicle_class': vehicle_class_name,
                'vehicle_confidence': vehicle_confidence,
                'vehicle_bbox': vehicle_bbox,
                'license_plate': license_text,
                'license_quality': quality_score,
                'frame_count': frame_count
            }
            self.license_output_log.append(log_entry)
            
            # å¦‚æœå¼€å¯è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºæ›´è¯¦ç»†çš„ä¿¡æ¯
            if self.debug:
                license_plate_info = vehicle_obj.get('license_plate', {})
                print(f"   â””â”€ è°ƒè¯•ä¿¡æ¯:")
                print(f"      è½¦ç‰Œæ£€æµ‹ç½®ä¿¡åº¦: {license_plate_info.get('confidence', 'N/A')}")
                print(f"      è½¦ç‰Œæ£€æµ‹æ¡†: {license_plate_info.get('bbox', 'N/A')}")
                print(f"      å­—ç¬¦è¯¦ç»†: {[c for c in license_text]}")
                print(f"      è¯†åˆ«å°è¯•ç»Ÿè®¡: {self.total_license_attempts} æ¬¡å°è¯•, {self.successful_license_recognitions} æ¬¡æˆåŠŸ")
                
                # å®‰å…¨çš„æˆåŠŸç‡è®¡ç®—
                if self.total_license_attempts > 0:
                    success_rate = (self.successful_license_recognitions/self.total_license_attempts*100)
                    print(f"      æˆåŠŸç‡: {success_rate:.1f}%")
                else:
                    print(f"      æˆåŠŸç‡: 0.0%")
                print("="*70)
    
    def _assess_license_quality(self, license_text: str) -> str:
        """è¯„ä¼°è½¦ç‰Œè´¨é‡"""
        if len(license_text) < 6:
            return "å·®"
        elif len(license_text) < 7:
            return "ä¸€èˆ¬"
        elif len(license_text) == 7:
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆä¸­å›½è½¦ç‰Œæ ¼å¼
            if len(license_text) >= 2:
                # ç¬¬ä¸€ä¸ªå­—ç¬¦åº”è¯¥æ˜¯ä¸­æ–‡çœä»½
                first_char = license_text[0]
                is_chinese = '\u4e00' <= first_char <= '\u9fff'
                if is_chinese:
                    return "ä¼˜ç§€"
                else:
                    return "è‰¯å¥½"
            return "è‰¯å¥½"
        else:
            return "è‰¯å¥½"
    
    def _visualize_frame_with_license(self, frame, tracked_objects):
        """å¯è§†åŒ–ï¼ŒåŒ…å«è½¦ç‰Œä¿¡æ¯"""
        for obj in tracked_objects:
            track_id = obj['track_id']
            x1, y1, x2, y2 = [int(c) for c in obj['bbox']]
            class_id = obj['class_id']
            confidence = obj.get('confidence', 0.0)
            hits = obj.get('hits', 0)
            license_plate = obj.get('license_plate', None)
            
            # ç¡®ä¿ç±»åˆ«IDæœ‰æ•ˆ
            if 0 <= class_id < len(self.class_names):
                color_idx = class_id % len(self.colors)
                color = self.colors[color_idx]
                class_name = self.class_names[class_id]
            else:
                color = (128, 128, 128)
                class_name = "æœªçŸ¥"
            
            # æ ¹æ®è¿½è¸ªç¨³å®šæ€§è°ƒæ•´é¢œè‰²äº®åº¦
            if hits < 3:
                color = tuple(int(c * 0.7) for c in color)
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            thickness = 3 if hits >= 5 else 2
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)
            
            # å‡†å¤‡æ ‡ç­¾æ–‡æœ¬
            label_lines = []
            label_lines.append(f"{class_name} ID:{track_id}")
            
            if license_plate:
                license_text = license_plate['text']
                label_lines.append(f"è½¦ç‰Œ: {license_text}")
            
            if self.debug:
                label_lines.append(f"H:{hits} C:{confidence:.2f}")
            
            # ç»˜åˆ¶å¤šè¡Œæ ‡ç­¾
            font_scale = 0.6
            font_thickness = 1
            line_height = 20
            
            # è®¡ç®—æ ‡ç­¾èƒŒæ™¯å¤§å°
            max_width = 0
            total_height = len(label_lines) * line_height + 5
            
            for line in label_lines:
                (tw, th), _ = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)
                max_width = max(max_width, tw)
            
            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
            label_bg_color = color
            cv2.rectangle(frame, (x1, y1 - total_height), (x1 + max_width + 10, y1), label_bg_color, -1)
            
            # ç»˜åˆ¶æ ‡ç­¾æ–‡æœ¬
            for i, line in enumerate(label_lines):
                text_y = y1 - total_height + (i + 1) * line_height - 5
                text_color = (255, 255, 255) if i == 0 else (255, 255, 0)  # è½¦ç‰Œä¿¡æ¯ç”¨é»„è‰²
                cv2.putText(frame, line, (x1 + 5, text_y),
                           cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, font_thickness, cv2.LINE_AA)
            
            # ä¸ºæ–°è¿½è¸ªæ·»åŠ ç‰¹æ®Šæ ‡è®°
            if hits < 3:
                cv2.circle(frame, (x1 + 10, y1 + 10), 5, (0, 255, 255), -1)
            
            # å¦‚æœæœ‰è½¦ç‰Œä¿¡æ¯ï¼Œåœ¨è½¦ç‰Œä½ç½®ç»˜åˆ¶å°æ¡†
            if license_plate and 'bbox' in license_plate:
                lp_bbox = license_plate['bbox']
                # å°†è½¦ç‰Œåæ ‡è½¬æ¢ä¸ºåŸå›¾åæ ‡ï¼ˆç›¸å¯¹äºè½¦è¾†è£å‰ªåŒºåŸŸçš„åç§»ï¼‰
                margin_x = int((x2 - x1) * 0.1)
                margin_y = int((y2 - y1) * 0.1)
                x1_expanded = max(0, x1 - margin_x)
                y1_expanded = max(0, y1 - margin_y)
                
                lp_x1 = int(x1_expanded + lp_bbox[0])
                lp_y1 = int(y1_expanded + lp_bbox[1])
                lp_x2 = int(x1_expanded + lp_bbox[2])
                lp_y2 = int(y1_expanded + lp_bbox[3])
                
                # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
                h, w = frame.shape[:2]
                lp_x1 = max(0, min(lp_x1, w-1))
                lp_y1 = max(0, min(lp_y1, h-1))
                lp_x2 = max(lp_x1+1, min(lp_x2, w))
                lp_y2 = max(lp_y1+1, min(lp_y2, h))
                
                # ç»˜åˆ¶è½¦ç‰Œæ¡†
                cv2.rectangle(frame, (lp_x1, lp_y1), (lp_x2, lp_y2), (0, 255, 255), 2)
    
    def process_video(self, video_path: str, output_path: str = None, save_stats: bool = True,
                     show_realtime: bool = True):
        """å¤„ç†è§†é¢‘ - æ”¯æŒè½¦ç‰Œè¯†åˆ«"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return

        # è·å–è§†é¢‘ä¿¡æ¯
        fps = int(cap.get(cv2.CAP_PROP_FPS)) if cap.get(cv2.CAP_PROP_FPS) > 0 else 25
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps} FPS, {total_frames if total_frames > 0 else 'N/A'} å¸§")
        print(f"è½¦ç‰Œè¯†åˆ«: {'å¯ç”¨' if self.license_enabled else 'ç¦ç”¨'}")
        print(f"è°ƒè¯•æ¨¡å¼: {'å¯ç”¨' if self.debug else 'ç¦ç”¨'}")
        
        # è®¾ç½®è¾“å‡ºè§†é¢‘
        out_writer = None
        if output_path:
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)
            fourcc_out = cv2.VideoWriter_fourcc(*'mp4v')
            out_writer = cv2.VideoWriter(output_path, fourcc_out, fps, (width, height))
            print(f"è¾“å‡ºè§†é¢‘å°†ä¿å­˜åˆ°: {output_path}")
        
        # åˆ›å»ºæ˜¾ç¤ºçª—å£
        if show_realtime:
            window_title = 'è½¦è¾†æ£€æµ‹ä¸è½¦ç‰Œè¯†åˆ«' if self.license_enabled else 'è½¦è¾†æ£€æµ‹'
            cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)
            cv2.resizeWindow(window_title, 1200, 800)
            print("æŒ‰ 'q' é”®é€€å‡ºï¼ŒæŒ‰ 'p' é”®æš‚åœ/ç»§ç»­ï¼ŒæŒ‰ 's' é”®æˆªå›¾")
        
        frame_count = 0
        processing_start_time = time.time()
        detection_count = 0
        license_recognition_count = 0
        paused = False
        last_stats_print = time.time()
        
        print("å¼€å§‹å¤„ç†è§†é¢‘...")
        print("="*80)
        
        try:
            while True:
                if not paused:
                    ret, frame = cap.read()
                    if not ret:
                        print("è§†é¢‘æµç»“æŸ.")
                        break
                    
                    frame_count += 1
                    current_loop_time = time.time()
                    
                    try:
                        # æ£€æµ‹è½¦è¾†
                        detections = self.detect_vehicles(frame.copy())
                        detection_count += len(detections)
                        
                        if self.debug and detections:
                            print(f"[DEBUG] ç¬¬{frame_count}å¸§æ£€æµ‹åˆ°{len(detections)}ä¸ªè½¦è¾†")
                        
                        # æ›´æ–°è¿½è¸ªå™¨
                        tracked_objects = self.tracker.update(detections, frame)
                        
                        if self.debug and tracked_objects:
                            print(f"[DEBUG] ç¬¬{frame_count}å¸§è¿½è¸ªåˆ°{len(tracked_objects)}ä¸ªå¯¹è±¡")
                        
                        # è½¦ç‰Œè¯†åˆ«ï¼ˆæ¯5å¸§è¯†åˆ«ä¸€æ¬¡ä»¥æé«˜æ€§èƒ½ï¼‰
                        if self.license_enabled and frame_count % 5 == 0:
                            if self.debug:
                                print(f"[DEBUG] ç¬¬{frame_count}å¸§å¼€å§‹è½¦ç‰Œè¯†åˆ«")
                            self._recognize_license_plates(frame, tracked_objects, frame_count)
                            license_recognition_count = len(self.recognized_licenses)
                        
                        # æ›´æ–°æµé‡è®¡æ•°
                        self.traffic_counter.update(tracked_objects, current_loop_time)
                        
                        # å¯è§†åŒ–ï¼ˆåŒ…å«è½¦ç‰Œä¿¡æ¯ï¼‰
                        self._visualize_frame_with_license(frame, tracked_objects)
                        
                    except Exception as e:
                        print(f"å¤„ç†ç¬¬ {frame_count} å¸§æ—¶å‡ºé”™: {e}")
                        if self.debug:
                            import traceback
                            traceback.print_exc()
                        continue
                    
                    # ç»˜åˆ¶è®¡æ•°çº¿å’Œç»Ÿè®¡ä¿¡æ¯
                    frame = self.traffic_counter.draw_counting_lines(frame)
                    self._draw_statistics_enhanced(frame, current_loop_time, license_recognition_count)
                    
                    # å†™å…¥è¾“å‡ºè§†é¢‘
                    if out_writer:
                        out_writer.write(frame)
                
                # å®æ—¶æ˜¾ç¤º
                if show_realtime:
                    if not paused:
                        window_title = 'è½¦è¾†æ£€æµ‹ä¸è½¦ç‰Œè¯†åˆ«' if self.license_enabled else 'è½¦è¾†æ£€æµ‹'
                        cv2.imshow(window_title, frame)
                    
                    key = cv2.waitKey(1) & 0xFF
                    if key == ord('q'):
                        print("ç”¨æˆ·è¯·æ±‚é€€å‡º")
                        break
                    elif key == ord('p'):
                        paused = not paused
                        print(f"{'æš‚åœ' if paused else 'ç»§ç»­'}æ’­æ”¾")
                    elif key == ord('s'):
                        screenshot_path = f"screenshot_with_license_{int(time.time())}.jpg"
                        cv2.imwrite(screenshot_path, frame)
                        print(f"æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
                
                # è¿›åº¦æŠ¥å‘Š
                if time.time() - last_stats_print > 10:
                    elapsed_time = time.time() - processing_start_time
                    avg_fps = frame_count / elapsed_time if elapsed_time > 0 else 0
                    progress = (frame_count / total_frames * 100) if total_frames > 0 else 0
                    
                    stats = self.traffic_counter.get_statistics()
                    
                    status_msg = f"è¿›åº¦: {frame_count}/{total_frames if total_frames > 0 else '~'} ({progress:.1f}%), " \
                               f"å¤„ç†FPS: {avg_fps:.1f}, æ€»è®¡æ•°: {stats['total_count']}"
                    
                    if self.license_enabled:
                        recognition_rate = (self.successful_license_recognitions / self.total_license_attempts * 100) if self.total_license_attempts > 0 else 0
                        status_msg += f", è½¦ç‰Œè¯†åˆ«: {len(self.recognized_licenses)} ä¸ª (æˆåŠŸç‡: {recognition_rate:.1f}%)"
                    
                    print(status_msg)
                    last_stats_print = time.time()

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
        
        finally:
            cap.release()
            if out_writer:
                out_writer.release()
            if show_realtime:
                cv2.destroyAllWindows()

        print("="*80)
        print(f"å¤„ç†å®Œæˆ! æ€»å¸§æ•°: {frame_count}, æ€»æ£€æµ‹æ•°: {detection_count}")
        if self.license_enabled:
            print(f"è½¦ç‰Œè¯†åˆ«å°è¯•: {self.total_license_attempts} æ¬¡")
            print(f"è½¦ç‰Œè¯†åˆ«æˆåŠŸ: {self.successful_license_recognitions} æ¬¡")
            recognition_rate = (self.successful_license_recognitions / self.total_license_attempts * 100) if self.total_license_attempts > 0 else 0
            print(f"è½¦ç‰Œè¯†åˆ«æˆåŠŸç‡: {recognition_rate:.1f}%")
            print(f"å”¯ä¸€è½¦ç‰Œæ€»æ•°: {len(self.recognized_licenses)}")
            self._print_license_summary()
        
        if save_stats:
            self._save_statistics_with_license(video_path, len(self.recognized_licenses))
        
        self._print_final_statistics()
        print(f"è§†é¢‘å¤„ç†å®Œæˆ! æ€»ç”¨æ—¶: {(time.time() - processing_start_time):.2f} ç§’.")
    
    def _print_license_summary(self):
        """æ‰“å°è½¦ç‰Œè¯†åˆ«æ±‡æ€»"""
        if not self.license_output_log:
            return
            
        print("\n" + "="*80)
        print("ğŸ·ï¸  è½¦ç‰Œè¯†åˆ«è¯¦ç»†æ±‡æ€»")
        print("="*80)
        
        for i, entry in enumerate(self.license_output_log, 1):
            print(f"{i:3d}. {entry['timestamp']} - è½¦è¾†ID: {entry['track_id']:03d} ({entry['vehicle_class']}) - è½¦ç‰Œ: {entry['license_plate']} - è´¨é‡: {entry['license_quality']}")
        
        print("="*80)
        print(f"æ€»è®¡è¯†åˆ«åˆ° {len(self.license_output_log)} ä¸ªä¸åŒçš„è½¦ç‰Œ")
        
        # æŒ‰è½¦è¾†ç±»å‹ç»Ÿè®¡
        type_stats = {}
        for entry in self.license_output_log:
            vehicle_type = entry['vehicle_class']
            type_stats[vehicle_type] = type_stats.get(vehicle_type, 0) + 1
        
        print("\nè½¦ç‰Œè¯†åˆ«æŒ‰è½¦è¾†ç±»å‹ç»Ÿè®¡:")
        for vtype, count in type_stats.items():
            print(f"  {vtype}: {count} ä¸ªè½¦ç‰Œ")
        
        print("="*80)
    
    def _draw_statistics_enhanced(self, frame: np.ndarray, current_time_sec: float, license_count: int):
        """ç»Ÿè®¡ä¿¡æ¯æ˜¾ç¤ºï¼ˆåŒ…å«è½¦ç‰Œè¯†åˆ«ä¿¡æ¯ï¼‰"""
        stats = self.traffic_counter.get_statistics()
        debug_info = stats['debug_info']
        
        # èƒŒæ™¯åŒºåŸŸ - æ ¹æ®æ˜¯å¦å¯ç”¨è½¦ç‰Œè¯†åˆ«è°ƒæ•´é«˜åº¦
        overlay_h = 260 if self.license_enabled else 200
        overlay_x, overlay_y, overlay_w = 10, 10, 500
        sub_img = frame[overlay_y:overlay_y+overlay_h, overlay_x:overlay_x+overlay_w]
        black_rect = np.zeros(sub_img.shape, dtype=np.uint8)
        res = cv2.addWeighted(sub_img, 0.5, black_rect, 0.5, 0)
        frame[overlay_y:overlay_y+overlay_h, overlay_x:overlay_x+overlay_w] = res

        text_color = (255, 255, 255)
        font_scale = 0.6
        line_height = 22
        current_y = overlay_y + line_height

        # æ€»è½¦è¾†æ•°
        cv2.putText(frame, f"Total Vehicles: {stats['total_count']}", 
                    (overlay_x + 10, current_y), cv2.FONT_HERSHEY_SIMPLEX, 
                    0.8, (0, 255, 0), 2, cv2.LINE_AA)
        current_y += line_height + 5
        
        # è½¦ç‰Œè¯†åˆ«ç»Ÿè®¡ - ä»…åœ¨å¯ç”¨æ—¶æ˜¾ç¤º
        if self.license_enabled:
            recognition_rate = (self.successful_license_recognitions / self.total_license_attempts * 100) if self.total_license_attempts > 0 else 0
            cv2.putText(frame, f"License Plates: {len(self.recognized_licenses)} ({recognition_rate:.1f}%)", 
                        (overlay_x + 10, current_y), cv2.FONT_HERSHEY_SIMPLEX, 
                        font_scale, (255, 255, 0), 1, cv2.LINE_AA)
            current_y += line_height
            
            # æ˜¾ç¤ºè¯†åˆ«å°è¯•æ¬¡æ•°
            cv2.putText(frame, f"Recognition Attempts: {self.total_license_attempts}", 
                        (overlay_x + 10, current_y), cv2.FONT_HERSHEY_SIMPLEX, 
                        0.5, (200, 200, 200), 1, cv2.LINE_AA)
            current_y += line_height
        
        # æµé‡
        flow_rate = stats['current_flow_rate_per_minute']
        cv2.putText(frame, f"Flow Rate: {flow_rate['total']}/min", 
                    (overlay_x + 10, current_y), cv2.FONT_HERSHEY_SIMPLEX, 
                    font_scale, text_color, 1, cv2.LINE_AA)
        current_y += line_height

        # æŒ‰ç±»åˆ«è®¡æ•°
        class_name_mapping = {
            'motorbike': 'Motorbike',
            'car': 'Car', 
            'bus': 'Bus',
            'truck': 'Truck'
        }
        
        class_counts_str = ", ".join([
            f"{class_name_mapping.get(self.class_names[cid], self.class_names[cid] if cid < len(self.class_names) else 'Unknown')}: {cnt}" 
            for cid, cnt in stats['count_by_class'].items()
        ])
        cv2.putText(frame, f"By Type: {class_counts_str}", 
                    (overlay_x + 10, current_y), cv2.FONT_HERSHEY_SIMPLEX, 
                    0.5, text_color, 1, cv2.LINE_AA)
        current_y += line_height

        # æ–¹å‘è®¡æ•°
        direction_mapping = {'up': 'Up', 'down': 'Down', 'both': 'Both'}
        direction_counts_str = ", ".join([
            f"{direction_mapping.get(direction, direction.capitalize())}: {count}" 
            for direction, count in stats['count_by_direction'].items()
        ])
        if direction_counts_str:
            cv2.putText(frame, f"Direction: {direction_counts_str}", 
                        (overlay_x + 10, current_y), cv2.FONT_HERSHEY_SIMPLEX, 
                        0.5, text_color, 1, cv2.LINE_AA)
            current_y += line_height

        # æŒ‰çº¿è®¡æ•°
        line_counts_str = ", ".join([
            f"Line{line_idx+1}: {count}" for line_idx, count in stats['count_by_line'].items()
        ])
        if line_counts_str:
            cv2.putText(frame, f"By Line: {line_counts_str}", 
                        (overlay_x + 10, current_y), cv2.FONT_HERSHEY_SIMPLEX, 
                        0.5, text_color, 1, cv2.LINE_AA)
            current_y += line_height
        
        # è°ƒè¯•ä¿¡æ¯
        cv2.putText(frame, f"Tracks: {debug_info['total_tracks']}, "
                        f"Crossing: {debug_info['crossing_attempts']}, "
                        f"Success: {debug_info['successful_counts']}", 
                    (overlay_x + 10, current_y), cv2.FONT_HERSHEY_SIMPLEX, 
                    0.4, (255, 255, 0), 1, cv2.LINE_AA)
        current_y += line_height
        
        # æ—¶é—´æˆ³
        time_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(current_time_sec))
        cv2.putText(frame, time_str, (overlay_x + 10, frame.shape[0] - 20), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1, cv2.LINE_AA)
    
    def _save_statistics_with_license(self, video_path: str, license_count: int):
        """ä¿å­˜åŒ…å«è½¦ç‰Œè¯†åˆ«çš„ç»Ÿè®¡æ•°æ®"""
        stats = self.traffic_counter.get_statistics()
        save_data = {
            'video_file': Path(video_path).name,
            'analysis_timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'configuration': {
                'model_input_size': self.config['model']['input_size'],
                'detection_conf_thresh': self.conf_threshold,
                'detection_iou_thresh': self.iou_threshold,
                'min_box_area': self.min_box_area,
                'max_box_area': self.max_box_area,
                'counting_lines': self.traffic_counter.counting_lines,
                'count_direction': self.traffic_counter.count_direction,
                'license_recognition_enabled': self.license_enabled,
                'debug_mode': self.debug
            },
            'traffic_statistics': stats
        }
        
        if self.license_enabled:
            recognition_rate = (self.successful_license_recognitions / self.total_license_attempts * 100) if self.total_license_attempts > 0 else 0
            save_data['license_plate_statistics'] = {
                'total_unique_plates': license_count,
                'total_recognition_attempts': self.total_license_attempts,
                'successful_recognitions': self.successful_license_recognitions,
                'recognition_success_rate': f"{recognition_rate:.2f}%",
                'unique_plate_rate': f"{(license_count / stats['total_count'] * 100):.1f}%" if stats['total_count'] > 0 else "0%",
                'recognized_plates': self.license_output_log
            }
        
        suffix = "_with_license_analysis" if self.license_enabled else "_vehicle_analysis"
        output_filename = Path(video_path).stem + suffix + ".json"
        output_dir = Path(self.config.get('training',{}).get('logs_dir', 'experiments/logs')) / "analysis_reports"
        output_dir.mkdir(parents=True, exist_ok=True)
        full_output_path = output_dir / output_filename

        try:
            with open(full_output_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=4, default=str)
            print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {full_output_path}")
        except Exception as e:
            print(f"ä¿å­˜ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
    
    def _print_final_statistics(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ç»“æœ"""
        stats = self.traffic_counter.get_statistics()
        print("\n" + "="*60)
        title = "æœ€ç»ˆç»Ÿè®¡ç»“æœï¼ˆåŒ…å«è½¦ç‰Œè¯†åˆ«ï¼‰" if self.license_enabled else "æœ€ç»ˆç»Ÿè®¡ç»“æœ"
        print(title)
        print("="*60)
        print(f"æ€»è½¦è¾†æ•°: {stats['total_count']}")
        
        if self.license_enabled:
            recognition_rate = (self.successful_license_recognitions / self.total_license_attempts * 100) if self.total_license_attempts > 0 else 0
            print(f"\nè½¦ç‰Œè¯†åˆ«ç»Ÿè®¡:")
            print(f"  è¯†åˆ«å°è¯•: {self.total_license_attempts} æ¬¡")
            print(f"  è¯†åˆ«æˆåŠŸ: {self.successful_license_recognitions} æ¬¡")
            print(f"  æˆåŠŸç‡: {recognition_rate:.1f}%")
            print(f"  å”¯ä¸€è½¦ç‰Œ: {len(self.recognized_licenses)} ä¸ª")
        
        print("\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
        for class_id, count in stats['count_by_class'].items():
            class_name = self.class_names[class_id] if 0 <= class_id < len(self.class_names) else f"æœªçŸ¥ç±»åˆ« {class_id}"
            print(f"  {class_name}: {count}")
        
        print("\næŒ‰æ–¹å‘ç»Ÿè®¡:")
        for direction, count in stats['count_by_direction'].items():
            print(f"  {direction.capitalize()}: {count}")
        
        flow_rate = stats['current_flow_rate_per_minute']
        print(f"\nå½“å‰æµé‡: {flow_rate['total']} è½¦è¾†/åˆ†é’Ÿ")
        print("="*60)

def main():
    parser = argparse.ArgumentParser(description='å¸¦è½¦ç‰Œè¯†åˆ«çš„è§†é¢‘è½¦è¾†åˆ†æç³»ç»Ÿ')
    parser.add_argument('--config', type=str, default='config.yaml', help='è½¦è¾†æ£€æµ‹é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--vehicle-model', type=str, required=True, help='è½¦è¾†æ£€æµ‹æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--license-yolo-config', type=str, required=True, help='è½¦ç‰ŒYOLOé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--license-yolo-model', type=str, required=True, help='è½¦ç‰ŒYOLOæ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--license-crnn-model', type=str, required=True, help='è½¦ç‰ŒCRNNæ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--video', type=str, required=True, help='è¾“å…¥è§†é¢‘è·¯å¾„')
    parser.add_argument('--output', type=str, help='è¾“å‡ºè§†é¢‘è·¯å¾„')
    parser.add_argument('--no-save-stats', action='store_true', help='ä¸ä¿å­˜ç»Ÿè®¡ç»“æœ')
    parser.add_argument('--no-display', action='store_true', help='ä¸æ˜¾ç¤ºå®æ—¶è§†é¢‘')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    files_to_check = [
        (args.config, "è½¦è¾†æ£€æµ‹é…ç½®æ–‡ä»¶"),
        (args.vehicle_model, "è½¦è¾†æ£€æµ‹æ¨¡å‹æ–‡ä»¶"),
        (args.license_yolo_config, "è½¦ç‰ŒYOLOé…ç½®æ–‡ä»¶"),
        (args.license_yolo_model, "è½¦ç‰ŒYOLOæ¨¡å‹æ–‡ä»¶"),
        (args.license_crnn_model, "è½¦ç‰ŒCRNNæ¨¡å‹æ–‡ä»¶"),
        (args.video, "è§†é¢‘æ–‡ä»¶")
    ]
    
    for file_path, name in files_to_check:
        if not Path(file_path).exists():
            print(f"{name}ä¸å­˜åœ¨: {file_path}")
            return
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = VideoVehicleAnalyzerWithLicensePlate(
        config_path=args.config,
        vehicle_model_path=args.vehicle_model,
        license_yolo_config=args.license_yolo_config,
        license_yolo_model=args.license_yolo_model,
        license_crnn_model=args.license_crnn_model,
        debug=args.debug
    )
    
    # å¤„ç†è§†é¢‘
    analyzer.process_video(
        video_path=args.video,
        output_path=args.output,
        save_stats=not args.no_save_stats,
        show_realtime=not args.no_display
    )

if __name__ == '__main__':
    main()